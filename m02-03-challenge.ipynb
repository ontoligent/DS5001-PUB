{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"307.2px"},"toc_section_display":true,"toc_window_display":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{},"version_major":2,"version_minor":0}},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14413362,"sourceType":"datasetVersion","datasetId":9205631}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# M02 Challenge\n\nDS 5001 Text as Data","metadata":{}},{"cell_type":"markdown","source":"## Purpose\n\nWe import a text using the  Clip, Chunk, and Split pattern.\n\nDemonstrate how to tokenize a raw text and map an OHCO onto the resulting dataframe of tokens.\n\nIn this notebook, we use the pattern from `M02_01` on a new text.","metadata":{}},{"cell_type":"markdown","source":"## Recipe\n\n### Create TOKEN table\n\n1. Inspect source text, taking note of where it begins and ends and the header patterns.\n2. Import the source text into a dataframe of line strings.\n3. Extract the title.\n4. Clip the cruft by using regexs for the beginning and end of the actual text.\n5. Chunk by using a regex for chapter headings, assign lines, and group.\n6. Split into paragraphs using new lines.\n7. Split into sentences using regex.\n8. Split into tokens using regex.\n\n## Create VOBAB table\n\n1. Get token value counts and save as data frame.","metadata":{}},{"cell_type":"markdown","source":"## Set Up","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"editable":true,"slideshow":{"slide_type":""},"tags":[]},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### Import Config","metadata":{}},{"cell_type":"code","source":"data_home = \"../input\"\noutput_dir = \"../working\"","metadata":{"editable":true,"slideshow":{"slide_type":""},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T18:22:12.084175Z","iopub.execute_input":"2026-01-06T18:22:12.084432Z","iopub.status.idle":"2026-01-06T18:22:12.093568Z","shell.execute_reply.started":"2026-01-06T18:22:12.084407Z","shell.execute_reply":"2026-01-06T18:22:12.092393Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"data_home","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T18:22:22.749995Z","iopub.execute_input":"2026-01-06T18:22:22.750730Z","iopub.status.idle":"2026-01-06T18:22:22.758738Z","shell.execute_reply.started":"2026-01-06T18:22:22.750690Z","shell.execute_reply":"2026-01-06T18:22:22.757755Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'../input'"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"text_file = f\"{data_home}/gutenberg/pg161.txt\"\n\n# The file you will create\ncsv_file = f\"{output_dir}/austen-sense-and-sensibility.csv\" ","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T18:22:25.739250Z","iopub.execute_input":"2026-01-06T18:22:25.739873Z","iopub.status.idle":"2026-01-06T18:22:25.745155Z","shell.execute_reply.started":"2026-01-06T18:22:25.739841Z","shell.execute_reply":"2026-01-06T18:22:25.744103Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"OHCO = ['chap_num', 'para_num', 'sent_num', 'token_num']","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T18:22:29.075986Z","iopub.execute_input":"2026-01-06T18:22:29.076282Z","iopub.status.idle":"2026-01-06T18:22:29.082288Z","shell.execute_reply.started":"2026-01-06T18:22:29.076258Z","shell.execute_reply":"2026-01-06T18:22:29.081213Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Import file into a dataframe","metadata":{}},{"cell_type":"markdown","source":"## Extract Title ","metadata":{}},{"cell_type":"markdown","source":"## Clip Cruft","metadata":{}},{"cell_type":"markdown","source":"## Chunk by chapter","metadata":{"tags":[]}},{"cell_type":"markdown","source":"### Find all chapter headers\n\nThe regex will depend on the source text. You need to investigate the source text to figure this out.","metadata":{}},{"cell_type":"markdown","source":"### Assign numbers to chapters","metadata":{}},{"cell_type":"markdown","source":"### Forward-fill chapter numbers to following text lines\n\n`ffill()` will replace null values with the previous non-null value.","metadata":{}},{"cell_type":"markdown","source":"### Clean up","metadata":{}},{"cell_type":"markdown","source":"### Group lines into chapters","metadata":{}},{"cell_type":"markdown","source":"## Split chapters into paragraphs \n\nWe use Pandas' convenient `.split()` method with `expand=True`, followed by `.stack()`.\nNote that this creates zero-based indexes.","metadata":{}},{"cell_type":"markdown","source":"## Split paragraphs into sentences","metadata":{}},{"cell_type":"markdown","source":"## Split sentences into tokens","metadata":{}},{"cell_type":"markdown","source":"## Extract Vocabulary","metadata":{}},{"cell_type":"markdown","source":"## Gathering by Content Object","metadata":{}},{"cell_type":"markdown","source":"## Save work to CSV\n\nThis is important -- will be used for homework.","metadata":{"editable":true,"slideshow":{"slide_type":""},"tags":[]}}]}